Ever since computers were invented, we have wondered whether they might be made to learn. If we could understand how to
program them to learn-to improve automatically with experience-the impact would be dramatic. Imagine comput-ers learning
from medical records which treatments are most effective for new diseases, houses learning from experience to optimize
energy costs based on the particular usage patterns of their occupants, or personal software assistants learn-ing the
evolving interests of their users in order to highlight especially relevant stories from the online morning newspaper. A
successful understanding of how to make computers learn would open up many new uses of computers and new levels of
competence and customization. And a detailed understanding of information-processing algorithms for machine learning
might lead to a better understanding of human learning abilities (and disabilities) as well. We do not yet know how to
make computers learn nearly as well as people learn. However, algorithms have been invented that are effective for
certain types of learning tasks, and a theoretical understanding of learning is beginning to emerge. Many practical
computer programs have been developed to exhibit use-ful types of learning, and significant commercial applications have
begun to ap-pear. For problems such as speech recognition, algorithms based on machine learning outperform all other
approaches that have been attempted to date. In the field known as data mining, machine learning algorithms are being
used rou-tinely to discover valuable knowledge from large commercial databases containing equipment maintenance records,
loan applications, financial transactions, medical records, and the like. As our understanding of computers continues to
mature, it seems inevitable that machine learning will play an increasingly central role in computer science and
computer technology. A few specific achievements provide a glimpse of the state of the art: pro-grams have been
developed that successfully learn to recognize spoken words (Waibel 1989; Lee 1989), predict recovery rates of pneumonia
patients (Cooper et al. 1997), detect fraudulent use of credit cards, drive autonomous vehicles on public highways
(Pomerleau 1989), and play games such as backgammon at levels approaching the performance of human world champions
(Tesauro 1992, 1995). Theoretical results have been developed that characterize the fundamental relationship among the
number of training examples observed, the number of hy-potheses under consideration, and the expected error in learned
hypotheses. We are beginning to obtain initial models of human and animal learning and to un- derstand their elationship
to learning algorithms developed for computers (e.g., Laird et al. 1986; Anderson 1991; Qin et al. 1992; Chi and Bassock
1989; Ahn and Brewer 1993). In applications, algorithms, theory, and studies of biological systems, the rate of progress
has increased significantly over the past decade. Sev-eral recent applications of machine learning are summarized in
Table 1.1. Langley and Simon (1995) and Rumelhart et al. (1994) survey additional applications of machine learning. This
book presents the field of machine learning, describing a variety of learning paradigms, lgorithms, theoretical results,
and applications. Machine learning is inherently a multidisciplinary field. It draws on results from artifi- cial
intelligence, probability and statistics, computational complexity theory, con- trol theory, information theory,
philosophy, psychology, neurobiology, and other fields. Table 1.2 summarizes key ideas from each of these fields that
impact the field of machine learning. While the material in this book is based on results from many diverse fields, the
reader need not be an expert in any of them. Key ideas are presented from these fields using a nonspecialist's
vocabulary, with unfamiliar terms and concepts introduced as the need arises. The first design choice we face is to
choose the type of training experience from which our system will learn. The type of training experience available can
have a significant impact on success or failure of the learner. One key attribute is whether the training experience
provides direct or indirect feedback regarding the choices made by the performance system. For example, in learning to
play checkers, the system might learn from direct training examples consisting of individual checkers board states and
the correct move for each. Alternatively, it might have available only indirect information consisting of the move
sequences and final outcomes of various games played. In this later case, information about the correctness of specific
moves early in the game must be inferred indirectly from the fact that the game was eventually won or lost. Here the
learner faces an additional problem of credit assignment, or determining the degree to which each move in the sequence
deserves credit or blame for the final outcome. Credit assignment can be a particularly difficult problem because the
game can be lost even when early moves are optimal, if these are followed later by poor moves. Hence, learning from
direct training feedback is typically easier than learning from indirect feedback. A second important attribute of the
training experience is the degree to which the learner controls the sequence of training examples. For example, the
learner might rely on the teacher to select informative board states and to provide the correct move for each.
Alternatively, the learner might itself propose board states that it finds particularly confusing and ask the teacher
for the correct move. Or the learner may have complete control over both the board states and (indirect) training
classifications, as it does when it learns by playing against itself with no teacher present. Notice in this last case
the learner may choose between experimenting with novel board states that it has not yet considered, or honing its skill
by playing minor variations of lines of play it currently finds most promising. Subsequent chapters consider a number of
settings for learning, including settings in training experience is provided by a random process outside the learner's
control, settings in which the learner may pose various types of queries to an expert teacher, and settings in which the
learner collects training examples by autonomously exploring its environment. A third important attribute of the
training experience is how well it repre- sents the distribution of examples over which the final system performance P
must be measured. In general, learning is most reliable when the training examples fol- low a distribution similar to
that of future test examples. In our checkers learning scenario, the performance metric P is the percent of games the
system wins in the world tournament. If its training experience E consists only of games played against itself, there is
an obvious danger that this training experience might not be fully representative of the distribution of situations over
which it will later be tested. For example, the learner might never encounter certain crucial board states that are very
likely to be played by the human checkers champion. In practice, it is often necessary to learn from a distribution of
examples that is somewhat different from those on which the final system will be evaluated (e.g., the world checkers
champion might not be interested in teaching the program!). Such situ- ations are problematic because mastery of one
distribution of examples will not necessary lead to strong performance over some other distribution. We shall see that
most current theory of machine learning rests on the crucial assumption that the distribution of training examples is
identical to the distribution of test ex- amples. Despite our need to make this assumption in order to obtain
theoretical results, it is important to keep in mind that this assumption must often be violated in practice. To proceed
with our design, let us decide that our system will train by playing games against itself. This has the advantage that
no external trainer need be present, and it therefore allows the system to generate as much training data as time
permits. We now have a fully specified learning task. The next design choice is to determine exactly what type of
knowledge will be learned and how this will be used by the performance program. Let us begin with a checkers-playing
program that can generate the legal moves from any board state. The program needs only to learn how to choose the best
move from among these legal moves. This learning task is representative of a large class of tasks for which the legal
moves that define some large search space are known a priori, but for which the best search strategy is not known. Many
optimization problems fall into this class, such as the problems of scheduling and controlling manufacturing processes
where the available manufacturing steps are well understood, but the best strategy for sequencing them is not. Given
this setting where we must learn to choose among the legal moves, the most obvious choice for the type of information to
be learned is a program, or function, that chooses the best move for any given board state. Let us call this function
ChooseMove and use the notation ChooseMove : B -+ M to indicate that this function accepts as input any board from the
set of legal board states B and produces as output some move from the set of legal moves M. Throughout our discussion of
machine learning we will find it useful to reduce the problem of improving performance P at task T to the problem of
learning some particu- lar targetfunction such as ChooseMove. The choice of the target function will therefore be a key
design choice. Although ChooseMove is an obvious choice for the target function in our example, this function will turn
out to be very difficult to learn given the kind of in- direct training experience available to our system. An
alternative target function- and one that will turn out to be easier to learn in this setting-is an evaluation function
that assigns a numerical score to any given board state. Let us call this target function V and again use the notation V
: B + 8 to denote that V maps any legal board state from the set B to some real value (we use 8 to denote the set of
real numbers). We intend for this target function V to assign higher scores to better board states. If the system can
successfully learn such a target function V , then it can easily use it to select the best move from any current board
position. This can be accomplished by generating the successor board state produced by every legal move, then using V to
choose the best successor state and therefore the best legal move. What exactly should be the value of the target
function V for any given board state? Of course any evaluation function that assigns higher scores to better board
states will do. Nevertheless, we will find it useful to define one particular target function V among the many that
produce optimal play. As we shall see, this will make it easier to design a training algorithm. Let us therefore define
the target value V ( b ) for an arbitrary board state b in B , as follows:
